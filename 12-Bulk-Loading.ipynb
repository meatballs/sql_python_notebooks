{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Loading\n",
    "\n",
    "Previously, we inserted sixty records into the readings table but often we have much bigger sets of data.\n",
    "\n",
    "The 'flight-data.csv' contains data for over 200 000 readings. Let's learn how to load that into our database.\n",
    "\n",
    "First we connect to our database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x15763d588>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "engine = sa.create_engine('sqlite:///flight.db')\n",
    "connection = engine.connect()\n",
    "connection.execute(\"PRAGMA foreign_keys=on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we load the contents of the file into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data = pd.read_csv('flight-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using the same technique as when we loaded the sixty records in Chapter 3.\n",
    "\n",
    "We defined an INSERT statement using bound parameters and then looped over our dataset and executed that statement once for each record.\n",
    "\n",
    "We'll do that again now but with the loop inside a function that we call so that we can time it.\n",
    "\n",
    "**NOTE** If you are using the online service, it is best not to run this example as it will take a significant length of time.\n",
    "\n",
    "**NOTE** There are some errors in the humidity readings due to sensor glitches. We have to use Python's min and max functions to ensure we don't breach the constraints we created on our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5min 38s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    INSERT INTO readings\n",
    "        (flight, ts, temp, pressure, humidity,\n",
    "        accel_x, accel_y, accel_z)\n",
    "    VALUES\n",
    "        ('hab1', ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "def load_data(connection, data):\n",
    "\n",
    "    for row in data.itertuples():\n",
    "        connection.execute(sql, (\n",
    "            row.timestamp, row.temp_h, row.pressure,\n",
    "            min(100, max(0, row.humidity)),\n",
    "            row.accel_x, row.accel_y, row.accel_z\n",
    "        ))\n",
    "\n",
    "connection.execute(\"DELETE FROM readings\")     \n",
    "%timeit -n1 -r1 load_data(connection, flight_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why this technique takes so long to run, we need to understand database transactions.\n",
    "\n",
    "When you ask the DBMS to make a change to a table using an INSERT, UPDATE or DELETE statement, it will guarantee that:\n",
    "\n",
    "* Atomicity - The entire change will either succeed or fail. It will never partially complete.\n",
    "* Consistency - Your data will always be consistent. Your table constraints will never be violated.\n",
    "* Isolation - Nobody else will see any of your changes until they have all completed.\n",
    "* Durability - Once your changes are complete, they have been entirely written to the underlying storage.\n",
    "\n",
    "These guarantees are known as ACID transactions.\n",
    "\n",
    "You can make as many changes as you want within a single transaction. If, for example, it is important that records inserted into two tables must either succeed or fail together (e.g. an invoice header and invoice detail table in an accounting system), then it would be sensible to make those inserts within a single transaction.\n",
    "\n",
    "Unless told otherwise, SQLite creates a transaction for each and every INSERT, UPDATE and DELETE statement and, because it has to guarantee the durability, it has to write to to the database file each. This repeated disk writing is the reason our first example is slow.\n",
    "\n",
    "Instead, let's implicitly specify that all our inserts should take place in a single transaction, and therefore only one write to disk.\n",
    "\n",
    "We specify a transaction using our connection object's `begin` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def load_data(connection, data):\n",
    "\n",
    "    with connection.begin():\n",
    "        for row in data.itertuples():\n",
    "            connection.execute(sql, (\n",
    "                row.timestamp, row.temp_h, row.pressure,\n",
    "                min(100, max(0, row.humidity)),\n",
    "                row.accel_x, row.accel_y, row.accel_z\n",
    "            ))\n",
    "            \n",
    "connection.execute(\"DELETE FROM readings\")\n",
    "%timeit -n1 -r1 load_data(connection, flight_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a huge drop in the amount of time it takes to execute this second example.\n",
    "\n",
    "But we can do better. At the moment, we are using a for loop to iterate over the pandas dataframe. Instead, let's use a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.02 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def load_data(connection, data):\n",
    "\n",
    "    data = [\n",
    "        (row.timestamp, row.temp_h, row.pressure,\n",
    "         min(100, max(0, row.humidity)),\n",
    "         row.accel_x, row.accel_y, row.accel_z)\n",
    "        for row in data.itertuples()\n",
    "    ]\n",
    "    with connection.begin():\n",
    "        connection.execute(sql, data)\n",
    "        \n",
    "connection.execute(\"DELETE FROM readings\")     \n",
    "%timeit -n1 -r1 load_data(connection, flight_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my laptop, that's now 100x faster than our first example and 4x faster than the second.\n",
    "\n",
    "Finally, although this tutorial focuses on learning SQL, it is worth noting that a pandas dataframe can be loaded directly into a database table using the `to_sql` method.\n",
    "\n",
    "We need to tidy up the dataframe first, but let's see how that performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.53 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "flight_data.drop(['temp_p', 'pitch', 'roll', 'yaw', 'mag_x', 'mag_y', 'mag_z', 'gyro_x', 'gyro_y', 'gyro_z'], axis=1, inplace=True)\n",
    "flight_data['humidity'] = flight_data['humidity'].clip(0, 100)\n",
    "flight_data = flight_data.rename(columns={'temp_h': 'temp', 'timestamp': 'ts'})\n",
    "flight_data['flight'] = 'hab1'\n",
    "\n",
    "connection.execute(\"DELETE FROM readings\")\n",
    "%timeit -n1 -r1 flight_data.to_sql('readings', connection, if_exists='append', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sql_python_tutorial]",
   "language": "python",
   "name": "conda-env-sql_python_tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
